{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05868709-fa4f-41f8-bd5f-9212280aee62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: taiwan, sensitive: sex\nTrain shape: (14999, 26) Val: (6001, 26) Test: (9000, 26)\nlambda=0.00 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.10 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.30 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.50 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=1.00 | Acc=0.741, F1=0.434, MCC=0.266\n\nBest lambda on validation (by F1): 0.0\n\n=== Test metrics with best lambda ===\naccuracy: 0.6868\nprecision: 0.2922\nrecall: 0.2923\nf1: 0.2922\nmcc: 0.0911\ndp_gap: 0.2577\ndp_p0: 0.3233\ndp_p1: 0.0657\nconfusion_matrix =\n [[5599 1410]\n [1409  582]]\nDataset: taiwan, sensitive: age\nTrain shape: (14999, 26) Val: (6001, 26) Test: (9000, 26)\nlambda=0.00 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.10 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.30 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=0.50 | Acc=0.741, F1=0.434, MCC=0.266\nlambda=1.00 | Acc=0.741, F1=0.434, MCC=0.266\n\nBest lambda on validation (by F1): 0.0\n\n=== Test metrics with best lambda ===\naccuracy: 0.6868\nprecision: 0.2922\nrecall: 0.2923\nf1: 0.2922\nmcc: 0.0911\ndp_gap: 0.2035\ndp_p0: 0.4046\ndp_p1: 0.2011\nconfusion_matrix =\n [[5599 1410]\n [1409  582]]\nMLP baseline | Dataset: taiwan, sensitive: sex\nTrain shape: (14999, 26) Val: (6001, 26) Test: (9000, 26)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== MLP Validation metrics ===\naccuracy: 0.7689\nprecision: 0.4690\nrecall: 0.3421\nf1: 0.3956\nmcc: 0.2619\ndp_gap: 0.0224\ndp_p0: 0.1525\ndp_p1: 0.1750\nconfusion_matrix =\n [[4160  514]\n [ 873  454]]\n\n=== MLP Test metrics ===\naccuracy: 0.7714\nprecision: 0.4771\nrecall: 0.3451\nf1: 0.4005\nmcc: 0.2690\ndp_gap: 0.0069\ndp_p0: 0.1573\ndp_p1: 0.1642\nconfusion_matrix =\n [[6256  753]\n [1304  687]]\nMLP baseline | Dataset: taiwan, sensitive: age\nTrain shape: (14999, 26) Val: (6001, 26) Test: (9000, 26)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== MLP Validation metrics ===\naccuracy: 0.7689\nprecision: 0.4690\nrecall: 0.3421\nf1: 0.3956\nmcc: 0.2619\ndp_gap: 0.0140\ndp_p0: 0.1599\ndp_p1: 0.1459\nconfusion_matrix =\n [[4160  514]\n [ 873  454]]\n\n=== MLP Test metrics ===\naccuracy: 0.7714\nprecision: 0.4771\nrecall: 0.3451\nf1: 0.4005\nmcc: 0.2690\ndp_gap: 0.0103\ndp_p0: 0.1541\ndp_p1: 0.1438\nconfusion_matrix =\n [[6256  753]\n [1304  687]]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-196705736324581>, line 547\u001B[0m\n",
       "\u001B[1;32m    531\u001B[0m mlp_val_taiwan_sex, mlp_test_taiwan_sex, mlp_model_taiwan_sex \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n",
       "\u001B[1;32m    532\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaiwan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    533\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    536\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n",
       "\u001B[1;32m    537\u001B[0m )\n",
       "\u001B[1;32m    539\u001B[0m mlp_val_taiwan_age, mlp_test_taiwan_age, mlp_model_taiwan_age \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n",
       "\u001B[1;32m    540\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaiwan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    541\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    544\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n",
       "\u001B[1;32m    545\u001B[0m )\n",
       "\u001B[0;32m--> 547\u001B[0m mlp_val_german_sex, mlp_test_german_sex, mlp_model_german_sex \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n",
       "\u001B[1;32m    548\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    549\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    550\u001B[0m     hidden_layer_sizes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m32\u001B[39m),\n",
       "\u001B[1;32m    551\u001B[0m     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n",
       "\u001B[1;32m    552\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n",
       "\u001B[1;32m    553\u001B[0m )\n",
       "\u001B[1;32m    555\u001B[0m mlp_val_german_age, mlp_test_german_age, mlp_model_german_age \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n",
       "\u001B[1;32m    556\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    557\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    560\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n",
       "\u001B[1;32m    561\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m<command-196705736324581>, line 489\u001B[0m, in \u001B[0;36mrun_mlp_baseline\u001B[0;34m(dataset_name, sensitive, hidden_layer_sizes, max_iter, random_state)\u001B[0m\n",
       "\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_mlp_baseline\u001B[39m(\n",
       "\u001B[1;32m    477\u001B[0m     dataset_name,\n",
       "\u001B[1;32m    478\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    482\u001B[0m ):\n",
       "\u001B[1;32m    483\u001B[0m     \u001B[38;5;66;03m# train a standard (non-boosted) MLPClassifier with 2 hidden layers and report validation + test metrics including fairness metrics.\u001B[39;00m\n",
       "\u001B[1;32m    484\u001B[0m     (\n",
       "\u001B[1;32m    485\u001B[0m         X_train, X_val, X_test,\n",
       "\u001B[1;32m    486\u001B[0m         y_train, y_val, y_test,\n",
       "\u001B[1;32m    487\u001B[0m         A_train, A_val, A_test,\n",
       "\u001B[1;32m    488\u001B[0m         scaler,\n",
       "\u001B[0;32m--> 489\u001B[0m     ) \u001B[38;5;241m=\u001B[39m load_dataset(dataset_name, sensitive\u001B[38;5;241m=\u001B[39msensitive)\n",
       "\u001B[1;32m    491\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLP baseline | Dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, sensitive: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msensitive\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    492\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_train\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVal:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_val\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_test\u001B[38;5;241m.\u001B[39mshape)\n",
       "\n",
       "File \u001B[0;32m<command-196705736324581>, line 320\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(name, sensitive, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_taiwan_dataset(sensitive\u001B[38;5;241m=\u001B[39msensitive, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[0;32m--> 320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_german_dataset(sensitive\u001B[38;5;241m=\u001B[39msensitive, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-196705736324581>, line 285\u001B[0m, in \u001B[0;36mload_german_dataset\u001B[0;34m(test_size, val_size, random_state, sensitive)\u001B[0m\n",
       "\u001B[1;32m    282\u001B[0m X \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(df_features, drop_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# train/val/test split\u001B[39;00m\n",
       "\u001B[0;32m--> 285\u001B[0m X_temp, X_test, y_temp, y_test, A_temp, A_test \u001B[38;5;241m=\u001B[39m train_test_split(\n",
       "\u001B[1;32m    286\u001B[0m     X, y, A,\n",
       "\u001B[1;32m    287\u001B[0m     test_size\u001B[38;5;241m=\u001B[39mtest_size,\n",
       "\u001B[1;32m    288\u001B[0m     stratify\u001B[38;5;241m=\u001B[39my,\n",
       "\u001B[1;32m    289\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n",
       "\u001B[1;32m    290\u001B[0m )\n",
       "\u001B[1;32m    292\u001B[0m val_rel \u001B[38;5;241m=\u001B[39m val_size \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m test_size)\n",
       "\u001B[1;32m    293\u001B[0m X_train, X_val, y_train, y_val, A_train, A_val \u001B[38;5;241m=\u001B[39m train_test_split(\n",
       "\u001B[1;32m    294\u001B[0m     X_temp, y_temp, A_temp,\n",
       "\u001B[1;32m    295\u001B[0m     test_size\u001B[38;5;241m=\u001B[39mval_rel,\n",
       "\u001B[1;32m    296\u001B[0m     stratify\u001B[38;5;241m=\u001B[39my_temp,\n",
       "\u001B[1;32m    297\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n",
       "\u001B[1;32m    298\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n",
       "\u001B[1;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n",
       "\u001B[1;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n",
       "\u001B[1;32m    214\u001B[0m         )\n",
       "\u001B[1;32m    215\u001B[0m     ):\n",
       "\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n",
       "\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n",
       "\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n",
       "\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n",
       "\u001B[1;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n",
       "\u001B[1;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n",
       "\u001B[1;32m    226\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2872\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n",
       "\u001B[1;32m   2868\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n",
       "\u001B[1;32m   2870\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n",
       "\u001B[0;32m-> 2872\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n",
       "\u001B[1;32m   2874\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n",
       "\u001B[1;32m   2876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n",
       "\u001B[1;32m   2877\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n",
       "\u001B[1;32m   2878\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n",
       "\u001B[1;32m   2879\u001B[0m     )\n",
       "\u001B[1;32m   2880\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1909\u001B[0m, in \u001B[0;36mBaseShuffleSplit.split\u001B[0;34m(self, X, y, groups)\u001B[0m\n",
       "\u001B[1;32m   1879\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n",
       "\u001B[1;32m   1880\u001B[0m \n",
       "\u001B[1;32m   1881\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1906\u001B[0m \u001B[38;5;124;03mto an integer.\u001B[39;00m\n",
       "\u001B[1;32m   1907\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1908\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n",
       "\u001B[0;32m-> 1909\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_indices(X, y, groups):\n",
       "\u001B[1;32m   1910\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2318\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit._iter_indices\u001B[0;34m(self, X, y, groups)\u001B[0m\n",
       "\u001B[1;32m   2316\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(y_indices)\n",
       "\u001B[1;32m   2317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmin(class_counts) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
       "\u001B[0;32m-> 2318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   2319\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe least populated class in y has only 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m member, which is too few. The minimum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m number of groups for any class cannot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2322\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be less than 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2323\u001B[0m     )\n",
       "\u001B[1;32m   2325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m<\u001B[39m n_classes:\n",
       "\u001B[1;32m   2326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   2327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe train_size = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m should be greater or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mequal to the number of classes = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_train, n_classes)\n",
       "\u001B[1;32m   2329\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-196705736324581>, line 547\u001B[0m\n\u001B[1;32m    531\u001B[0m mlp_val_taiwan_sex, mlp_test_taiwan_sex, mlp_model_taiwan_sex \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n\u001B[1;32m    532\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaiwan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    533\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    536\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    537\u001B[0m )\n\u001B[1;32m    539\u001B[0m mlp_val_taiwan_age, mlp_test_taiwan_age, mlp_model_taiwan_age \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n\u001B[1;32m    540\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtaiwan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    541\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    544\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    545\u001B[0m )\n\u001B[0;32m--> 547\u001B[0m mlp_val_german_sex, mlp_test_german_sex, mlp_model_german_sex \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n\u001B[1;32m    548\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    549\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    550\u001B[0m     hidden_layer_sizes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m32\u001B[39m),\n\u001B[1;32m    551\u001B[0m     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[1;32m    552\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    553\u001B[0m )\n\u001B[1;32m    555\u001B[0m mlp_val_german_age, mlp_test_german_age, mlp_model_german_age \u001B[38;5;241m=\u001B[39m run_mlp_baseline(\n\u001B[1;32m    556\u001B[0m     dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    557\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    560\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    561\u001B[0m )\n",
        "File \u001B[0;32m<command-196705736324581>, line 489\u001B[0m, in \u001B[0;36mrun_mlp_baseline\u001B[0;34m(dataset_name, sensitive, hidden_layer_sizes, max_iter, random_state)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_mlp_baseline\u001B[39m(\n\u001B[1;32m    477\u001B[0m     dataset_name,\n\u001B[1;32m    478\u001B[0m     sensitive\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    482\u001B[0m ):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;66;03m# train a standard (non-boosted) MLPClassifier with 2 hidden layers and report validation + test metrics including fairness metrics.\u001B[39;00m\n\u001B[1;32m    484\u001B[0m     (\n\u001B[1;32m    485\u001B[0m         X_train, X_val, X_test,\n\u001B[1;32m    486\u001B[0m         y_train, y_val, y_test,\n\u001B[1;32m    487\u001B[0m         A_train, A_val, A_test,\n\u001B[1;32m    488\u001B[0m         scaler,\n\u001B[0;32m--> 489\u001B[0m     ) \u001B[38;5;241m=\u001B[39m load_dataset(dataset_name, sensitive\u001B[38;5;241m=\u001B[39msensitive)\n\u001B[1;32m    491\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMLP baseline | Dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, sensitive: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msensitive\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_train\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVal:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_val\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest:\u001B[39m\u001B[38;5;124m\"\u001B[39m, X_test\u001B[38;5;241m.\u001B[39mshape)\n",
        "File \u001B[0;32m<command-196705736324581>, line 320\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(name, sensitive, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_taiwan_dataset(sensitive\u001B[38;5;241m=\u001B[39msensitive, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgerman\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_german_dataset(sensitive\u001B[38;5;241m=\u001B[39msensitive, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-196705736324581>, line 285\u001B[0m, in \u001B[0;36mload_german_dataset\u001B[0;34m(test_size, val_size, random_state, sensitive)\u001B[0m\n\u001B[1;32m    282\u001B[0m X \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(df_features, drop_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# train/val/test split\u001B[39;00m\n\u001B[0;32m--> 285\u001B[0m X_temp, X_test, y_temp, y_test, A_temp, A_test \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m    286\u001B[0m     X, y, A,\n\u001B[1;32m    287\u001B[0m     test_size\u001B[38;5;241m=\u001B[39mtest_size,\n\u001B[1;32m    288\u001B[0m     stratify\u001B[38;5;241m=\u001B[39my,\n\u001B[1;32m    289\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n\u001B[1;32m    290\u001B[0m )\n\u001B[1;32m    292\u001B[0m val_rel \u001B[38;5;241m=\u001B[39m val_size \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m test_size)\n\u001B[1;32m    293\u001B[0m X_train, X_val, y_train, y_val, A_train, A_val \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m    294\u001B[0m     X_temp, y_temp, A_temp,\n\u001B[1;32m    295\u001B[0m     test_size\u001B[38;5;241m=\u001B[39mval_rel,\n\u001B[1;32m    296\u001B[0m     stratify\u001B[38;5;241m=\u001B[39my_temp,\n\u001B[1;32m    297\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n\u001B[1;32m    298\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    214\u001B[0m         )\n\u001B[1;32m    215\u001B[0m     ):\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    226\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2872\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2868\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[1;32m   2870\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m-> 2872\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[1;32m   2874\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m   2877\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m   2878\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[1;32m   2879\u001B[0m     )\n\u001B[1;32m   2880\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1909\u001B[0m, in \u001B[0;36mBaseShuffleSplit.split\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m   1879\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[1;32m   1880\u001B[0m \n\u001B[1;32m   1881\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1906\u001B[0m \u001B[38;5;124;03mto an integer.\u001B[39;00m\n\u001B[1;32m   1907\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1908\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n\u001B[0;32m-> 1909\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_indices(X, y, groups):\n\u001B[1;32m   1910\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2318\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit._iter_indices\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m   2316\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(y_indices)\n\u001B[1;32m   2317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmin(class_counts) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m-> 2318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2319\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe least populated class in y has only 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m member, which is too few. The minimum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m number of groups for any class cannot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2322\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be less than 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2323\u001B[0m     )\n\u001B[1;32m   2325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m<\u001B[39m n_classes:\n\u001B[1;32m   2326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe train_size = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m should be greater or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mequal to the number of classes = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_train, n_classes)\n\u001B[1;32m   2329\u001B[0m     )\n",
        "\u001B[0;31mValueError\u001B[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# helpers\n",
    "def demographic_parity_gap(y_pred, A):\n",
    "    '''\n",
    "    todo: function description\n",
    "    '''\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    A = np.asarray(A)\n",
    "\n",
    "    mask0 = (A == 0)\n",
    "    mask1 = (A == 1)\n",
    "\n",
    "    p0 = y_pred[mask0].mean()\n",
    "    p1 = y_pred[mask1].mean()\n",
    "\n",
    "    return abs(p1 - p0), p0, p1\n",
    "\n",
    "def compute_metrics(y_true, y_pred, A):\n",
    "    '''\n",
    "    todo: function description\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    dp_gap, p0, p1 = demographic_parity_gap(y_pred, A)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mcc\": mcc,\n",
    "        \"dp_gap\": dp_gap,\n",
    "        \"dp_p0\": p0,\n",
    "        \"dp_p1\": p1,\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "''' \n",
    "============================================================\n",
    " AdaBoost implementation w/ Perceptron\n",
    "============================================================\n",
    "'''\n",
    "class AdaBoostClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, \n",
    "                 base_estimator=None, \n",
    "                 n_estimators=50, \n",
    "                 learning_rate=1.0,\n",
    "                 fairness_lambda=0.0,\n",
    "                 dp_tolerance=0.0,\n",
    "                 random_state=None):\n",
    "\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.fairness_lambda = fairness_lambda\n",
    "        self.dp_tolerance = dp_tolerance\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "        self.classes_ = None\n",
    "\n",
    "    def check_binary(self, y):\n",
    "        y = np.asarray(y)\n",
    "        classes = np.unique(y)\n",
    "        if len(classes) != 2:\n",
    "            raise ValueError(\"Binary classification only.\")\n",
    "\n",
    "        self.classes_ = classes\n",
    "        return (y == classes[1]).astype(int)\n",
    "\n",
    "    def fit(self, X, y, A=None):\n",
    "        '''\n",
    "        fit with fairness modification using demographic parity gap.\n",
    "        A is the sensitive attribute (0/1).\n",
    "        '''\n",
    "        if self.base_estimator is None:\n",
    "            raise ValueError(\"base_estimator must be provided.\")\n",
    "\n",
    "        if A is None:\n",
    "            raise ValueError(\"Sensitive attribute A must be provided for fairness-aware training.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        A = np.asarray(A)\n",
    "\n",
    "        y01 = self.check_binary(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        sample_weight = np.ones(n_samples) / n_samples\n",
    "\n",
    "        rng = np.random.RandomState(self.random_state) # random states\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            est = clone(self.base_estimator)\n",
    "            est.fit(X, y01, sample_weight=sample_weight)\n",
    "\n",
    "            y_pred = est.predict(X).astype(int)\n",
    "            incorrect = (y_pred != y01).astype(float)\n",
    "\n",
    "            err = np.dot(sample_weight, incorrect) / sample_weight.sum()\n",
    "            err = np.clip(err, 1e-10, 1 - 1e-10)\n",
    "\n",
    "            # modified @ 12/07\n",
    "            # fairness penalty\n",
    "            # dp_gap is absolute difference of positive rates between A=0 and A=1\n",
    "            dp_gap, _, _ = demographic_parity_gap(y_pred, A)\n",
    "\n",
    "            # if dp_gap > dp_tolerance, then we add a penalty\n",
    "            fairness_penalty = self.fairness_lambda * max(0.0, dp_gap - self.dp_tolerance)\n",
    "\n",
    "            # combined error\n",
    "            combined_err = err + fairness_penalty\n",
    "            combined_err = np.clip(combined_err, 1e-10, 1 - 1e-10)\n",
    "            # modified @ 12/07 || end\n",
    "\n",
    "            # alpha = self.learning_rate * 0.5 * np.log((1 - err) / err)\n",
    "            alpha = 0.5 * np.log((1 - combined_err) / combined_err) * self.learning_rate\n",
    "\n",
    "            if alpha <= 0:\n",
    "                break\n",
    "\n",
    "            # sample_weight *= np.exp(alpha * incorrect.astype(float))\n",
    "            # sample_weight /= sample_weight.sum()\n",
    "            sample_weight *= np.exp(-alpha * (2 * y01 - 1) * (2 * y_pred - 1))\n",
    "            sample_weight /= sample_weight.sum()\n",
    "\n",
    "            self.estimators_.append(est)\n",
    "            self.estimator_weights_.append(alpha)\n",
    "\n",
    "        self.estimator_weights_ = np.array(self.estimator_weights_)\n",
    "        return self\n",
    "\n",
    "    def scores(self, X):\n",
    "        X = np.asarray(X)\n",
    "        scores = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, est in zip(self.estimator_weights_, self.estimators_):\n",
    "            pred = est.predict(X).astype(int)\n",
    "            pred_pm1 = 2 * pred - 1\n",
    "            scores += alpha * pred_pm1\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.scores(X)\n",
    "        y01 = (scores >= 0).astype(int)\n",
    "        return np.where(y01 == 1, self.classes_[1], self.classes_[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = self.scores(X)\n",
    "        p_pos = 1 / (1 + np.exp(-2 * scores))\n",
    "        return np.vstack([1 - p_pos, p_pos]).T\n",
    "\n",
    "# data pre-processing\n",
    "DATA_DIR = \"/Workspace/Users/alexandra.mangune@gmail.com/Masters/AI201/Mini-Project/\" # todo: modify this path to match your folder path\n",
    "\n",
    "def load_taiwan_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the Taiwan dataset (UCI Credit Card).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"UCI_Credit_Card.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # Target\n",
    "    y = df[\"default.payment.next.month\"].values\n",
    "\n",
    "    # Sensitive attributes\n",
    "    df[\"SEX_BIN\"] = df[\"SEX\"].map({1: 1, 2: 0})\n",
    "    df[\"AGE_NUM\"] = df[\"AGE\"]\n",
    "    df[\"AGE_GROUP\"] = pd.cut(\n",
    "        df[\"AGE\"], bins=[0, 29, 39, 49, 59, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"SEX_BIN\"].values\n",
    "    else:\n",
    "        A = df[\"AGE_GROUP\"].values\n",
    "\n",
    "    df_features = df.drop(columns=[\"default.payment.next.month\"])\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    # train/validate/test split\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_german_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the German dataset (German Credit).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"german_credit_data.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # if \"risk\" in df.columns:\n",
    "    #     df[\"risk_bin\"] = df[\"risk\"].map({\"good\": 0, \"bad\": 1})\n",
    "    #     y = df[\"risk_bin\"].values\n",
    "    #     drop_tgt = [\"risk\", \"risk_bin\"]\n",
    "    # elif \"credit_risk\" in df.columns:\n",
    "    #     le = LabelEncoder()\n",
    "    #     y = le.fit_transform(df[\"credit_risk\"])\n",
    "    #     drop_tgt = [\"credit_risk\"]\n",
    "    # else:\n",
    "    #     raise ValueError(\"Cannot find target column in German dataset\")\n",
    "\n",
    "    # if \"sex\" in df.columns:\n",
    "    #     df[\"sex_bin\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "    #     # df[\"sex_bin\"] = df[\"Sex\"].map({\"male\": 1, \"female\": 0})\n",
    "    # else:\n",
    "    #     raise ValueError(\"Expected 'sex' column in German dataset\")\n",
    "\n",
    "    y = df[\"credit amount\"].values\n",
    "\n",
    "    df[\"sex_bin\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "    df[\"age_num\"] = df[\"age\"]\n",
    "    df[\"age_group\"] = pd.cut(\n",
    "        df[\"age\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "    # df[\"age_num\"] = df[\"Age\"]\n",
    "    # df[\"age_group\"] = pd.cut(\n",
    "    #     df[\"Age\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    # ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"sex_bin\"].values\n",
    "    else:\n",
    "        A = df[\"age_group\"].values\n",
    "\n",
    "    # drop_cols = drop_tgt + [\"sex\", \"age\"]\n",
    "    drop_cols = [\"sex\", \"age\"]\n",
    "    df_features = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    # train/val/test split\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_dataset(name, sensitive=\"sex\", **kwargs):\n",
    "    '''\n",
    "    Call either load_taiwan_dataset() or load_german_dataset() functions to load the dataset.\n",
    "    '''\n",
    "    if name.lower() == 'taiwan':\n",
    "        return load_taiwan_dataset(sensitive=sensitive, **kwargs)\n",
    "    elif name.lower() == 'german':\n",
    "        return load_german_dataset(sensitive=sensitive, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "\n",
    "# run training and testing sets; validate the outputs\n",
    "def run_experiment(\n",
    "    dataset_name,\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=50,\n",
    "    dp_tolerance=0.05, # for fairness modifications\n",
    "):\n",
    "    '''\n",
    "    runs the experiment for a given dataset and sensitive attribute.\n",
    "    '''\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test, # this is for the fairness modification\n",
    "        scaler,\n",
    "    ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "    print(f\"Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "    print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    # use a Perceptron as the base learner instead of a decision tree stump\n",
    "    base = Perceptron(\n",
    "        max_iter=1000,\n",
    "        eta0=0.01,\n",
    "        random_state=0,\n",
    "        tol=1e-3,\n",
    "        fit_intercept=True\n",
    "    )\n",
    "\n",
    "    val_results = []\n",
    "\n",
    "    for lam in lambdas:\n",
    "        model = AdaBoostClassifier(\n",
    "            base_estimator=base,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=1.0,\n",
    "            fairness_lambda=lam, # for fairness modifications\n",
    "            dp_tolerance=dp_tolerance # for fairness modifications\n",
    "        )\n",
    "        # fairness-aware fit uses A_train\n",
    "        model.fit(X_train, y_train, A_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "        metrics[\"lambda\"] = lam\n",
    "        val_results.append(metrics)\n",
    "\n",
    "        print(\n",
    "            f\"lambda={lam:.2f} | \"\n",
    "            f\"Acc={metrics['accuracy']:.3f}, F1={metrics['f1']:.3f}, \"\n",
    "            f\"MCC={metrics['mcc']:.3f}\"\n",
    "        )\n",
    "\n",
    "    best_idx = np.argmax([m[\"f1\"] for m in val_results])\n",
    "    best_lam = val_results[best_idx][\"lambda\"]\n",
    "    print(\"\\nBest lambda on validation (by F1):\", best_lam)\n",
    "\n",
    "    X_tr_full = pd.concat([X_train, X_val], axis=0)\n",
    "    y_tr_full = np.concatenate([y_train, y_val])\n",
    "    A_tr_full = np.concatenate([A_train, A_val])\n",
    "\n",
    "    best_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=best_lam,\n",
    "        dp_tolerance=dp_tolerance,\n",
    "    )\n",
    "    best_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred, A_test)\n",
    "    print(\"\\n=== Test metrics with best lambda ===\")\n",
    "    for k, v in test_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return val_results, test_metrics\n",
    "\n",
    "# plot figures\n",
    "def plot_tradeoff(val_results, title_suffix=\"\"):\n",
    "    '''\n",
    "    Plot the tradeoff between accuracy and demographic parity gap.\n",
    "    '''\n",
    "    lambdas = [m[\"lambda\"] for m in val_results]\n",
    "    acc = [m[\"accuracy\"] for m in val_results]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lambdas, acc, marker=\"o\")\n",
    "    plt.xlabel(\"lambda (fairness_lambda)\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.title(f\"Accuracy vs lambda {title_suffix}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "# run experiments\n",
    "\n",
    "''' \n",
    "============================================================\n",
    " Run AdaBoost experiments (Perceptron base learner)\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=40,\n",
    "    dp_tolerance=0.05, # for fairness modifications\n",
    ")\n",
    "# plot_tradeoff(val_res_taiwan, title_suffix=\"(Taiwan, Sex)\")\n",
    "\n",
    "# Taiwan dataset fairness on age\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"age\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=40,\n",
    "    dp_tolerance=0.05, # for fairness modifications\n",
    ")\n",
    "# plot_tradeoff(val_res_taiwan, title_suffix=\"(Taiwan, Age)\")\n",
    "\n",
    "# TODO: there's an error here!\n",
    "# # German dataset fairness on sex\n",
    "# val_res_german, test_german = run_experiment(\n",
    "#     dataset_name=\"german\",\n",
    "#     sensitive=\"sex\",\n",
    "#     lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "#     n_estimators=40,\n",
    "#     dp_tolerance=0.05, # for fairness modifications\n",
    "# )\n",
    "# # plot_tradeoff(val_res_german, title_suffix=\"(German, Sex)\")\n",
    "\n",
    "# # German dataset fairness on age\n",
    "# val_res_german, test_german = run_experiment(\n",
    "#     dataset_name=\"german\",\n",
    "#     sensitive=\"age\",\n",
    "#     lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "#     n_estimators=40,\n",
    "#     dp_tolerance=0.05, # for fairness modifications\n",
    "# )\n",
    "# # plot_tradeoff(val_res_german, title_suffix=\"(German, Age)\")\n",
    "\n",
    "\n",
    "'''\n",
    "============================================================\n",
    "Baseline ANN model using MLPClassifier (2 hidden layers)\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "def run_mlp_baseline(\n",
    "    dataset_name,\n",
    "    sensitive=\"sex\",\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    "):\n",
    "    # train a standard (non-boosted) MLPClassifier with 2 hidden layers and report validation + test metrics including fairness metrics.\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "    print(f\"MLP baseline | Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "    print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Validation metrics\n",
    "    y_val_pred = mlp.predict(X_val)\n",
    "    val_metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "\n",
    "    # Test metrics\n",
    "    y_test_pred = mlp.predict(X_test)\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred, A_test)\n",
    "\n",
    "    print(\"\\n=== MLP Validation metrics ===\")\n",
    "    for k, v in val_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== MLP Test metrics ===\")\n",
    "    for k, v in test_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return val_metrics, test_metrics, mlp\n",
    "\n",
    "# sample MLP runs (can remove later)\n",
    "\n",
    "mlp_val_taiwan_sex, mlp_test_taiwan_sex, mlp_model_taiwan_sex = run_mlp_baseline(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"sex\",\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "mlp_val_taiwan_age, mlp_test_taiwan_age, mlp_model_taiwan_age = run_mlp_baseline(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"age\",\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "mlp_val_german_sex, mlp_test_german_sex, mlp_model_german_sex = run_mlp_baseline(\n",
    "    dataset_name=\"german\",\n",
    "    sensitive=\"sex\",\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "mlp_val_german_age, mlp_test_german_age, mlp_model_german_age = run_mlp_baseline(\n",
    "    dataset_name=\"german\",\n",
    "    sensitive=\"age\",\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    max_iter=500,\n",
    "    random_state=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) [AI201 Mini Project] Lopez, Alyanna & Mangune, Alexandra v3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}